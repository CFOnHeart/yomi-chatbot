import sqlite3
import json
import pickle
import numpy as np
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import uuid
import os

try:
    import faiss
except ImportError:
    print("âš ï¸ FAISS library not installed. Please install it with: pip install faiss-cpu")
    faiss = None

class FAISSDocumentDatabase:
    def __init__(self, db_path: str = "documents.db", vector_path: str = "vectors.index"):
        self.db_path = db_path
        self.vector_path = vector_path
        self.metadata_path = vector_path.replace('.index', '_metadata.pkl')
        
        # FAISSç´¢å¼•
        self.index = None
        self.dimension = 1536  # Azure OpenAI embedding dimension
        self.document_ids = []  # ä¿å­˜æ–‡æ¡£IDçš„é¡ºåº
        if not os.path.exists(self.db_path):
            print (f"âš ï¸ æ•°æ®åº“æ–‡ä»¶ {self.db_path} ä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–æ•°æ®åº“...")
            self.init_database()
        self.init_faiss_index()

    def init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“ï¼Œåªå­˜å‚¨å…ƒæ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºæ–‡æ¡£å…ƒæ•°æ®è¡¨ï¼ˆä¸å­˜å‚¨embeddingï¼‰
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                file_path TEXT,
                file_type TEXT,
                chunk_index INTEGER DEFAULT 0,
                start_line INTEGER,
                end_line INTEGER,
                word_count INTEGER,
                char_count INTEGER,
                language TEXT DEFAULT 'zh',
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                -- æ‰©å±•å­—æ®µ
                metadata TEXT,
                tags TEXT,
                category TEXT,
                priority INTEGER DEFAULT 0,
                status TEXT DEFAULT 'active',
                source TEXT,
                author TEXT,
                version TEXT DEFAULT '1.0',
                parent_id TEXT,
                -- æœç´¢ç›¸å…³
                search_keywords TEXT,
                summary TEXT,
                -- ä¸šåŠ¡ç›¸å…³æ‰©å±•å­—æ®µ
                business_unit TEXT,
                access_level TEXT DEFAULT 'public',
                expiry_date TIMESTAMP,
                custom_field1 TEXT,
                custom_field2 TEXT,
                custom_field3 TEXT,
                -- FAISSç´¢å¼•ä½ç½®
                faiss_index INTEGER  -- åœ¨FAISSç´¢å¼•ä¸­çš„ä½ç½®
            )
        ''')
        
        # åˆ›å»ºæ–‡æ¡£é›†åˆè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS document_collections (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                metadata TEXT
            )
        ''')
        
        # åˆ›å»ºæ–‡æ¡£ä¸é›†åˆçš„å…³è”è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS document_collection_mapping (
                document_id TEXT NOT NULL,
                collection_id TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                PRIMARY KEY (document_id, collection_id),
                FOREIGN KEY (document_id) REFERENCES documents (id),
                FOREIGN KEY (collection_id) REFERENCES document_collections (id)
            )
        ''')
        
        # åˆ›å»ºæœç´¢æ—¥å¿—è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_logs (
                id TEXT PRIMARY KEY,
                query TEXT NOT NULL,
                results_count INTEGER,
                search_type TEXT,
                execution_time FLOAT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                session_id TEXT,
                metadata TEXT
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_documents_file_path ON documents(file_path)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_documents_created_at ON documents(created_at)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_documents_category ON documents(category)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_documents_status ON documents(status)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_documents_tags ON documents(tags)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_documents_source ON documents(source)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_search_logs_query ON search_logs(query)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_search_logs_session ON search_logs(session_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_documents_faiss_index ON documents(faiss_index)')
        
        # åˆ›å»ºå…¨æ–‡æœç´¢ç´¢å¼•
        cursor.execute('''
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                id, title, content, summary, search_keywords,
                content='documents',
                content_rowid='rowid'
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def init_faiss_index(self):
        """åˆå§‹åŒ–FAISSç´¢å¼•"""
        if faiss is None:
            print("âš ï¸ FAISS not available, falling back to traditional search only")
            return
        
        # å°è¯•åŠ è½½ç°æœ‰çš„FAISSç´¢å¼•
        if Path(self.vector_path).exists() and Path(self.metadata_path).exists():
            try:
                self.index = faiss.read_index(self.vector_path)
                
                # åŠ è½½å…ƒæ•°æ®
                with open(self.metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                    self.document_ids = metadata.get('document_ids', [])
                    self.dimension = metadata.get('dimension', 1536)
                
                print(f"âœ… åŠ è½½FAISSç´¢å¼•: {self.index.ntotal} ä¸ªå‘é‡")
                
            except Exception as e:
                print(f"âš ï¸ åŠ è½½FAISSç´¢å¼•å¤±è´¥: {e}")
                self._create_new_index()
        else:
            self._create_new_index()
    
    def _create_new_index(self):
        """åˆ›å»ºæ–°çš„FAISSç´¢å¼•"""
        if faiss is None:
            return
        
        # åˆ›å»ºFAISSç´¢å¼• (ä½¿ç”¨L2è·ç¦»çš„å¹³é¢ç´¢å¼•)
        self.index = faiss.IndexFlatL2(self.dimension)
        self.document_ids = []
        
        # å¦‚æœæ•°æ®åº“ä¸­å·²æœ‰æ–‡æ¡£ï¼Œé‡æ–°æ„å»ºç´¢å¼•
        self._rebuild_index()
    
    def _rebuild_index(self):
        """é‡å»ºFAISSç´¢å¼•"""
        if faiss is None:
            return
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # è·å–æ‰€æœ‰æœ‰æ•ˆæ–‡æ¡£
        cursor.execute('''
            SELECT id, content FROM documents 
            WHERE status = 'active' 
            ORDER BY created_at
        ''')
        
        documents = cursor.fetchall()
        conn.close()
        
        if not documents:
            return
        
        print(f"ğŸ”„ é‡å»ºFAISSç´¢å¼•ï¼Œå¤„ç† {len(documents)} ä¸ªæ–‡æ¡£...")
        
        # è¿™é‡Œéœ€è¦é‡æ–°ç”Ÿæˆembeddingsï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦è°ƒç”¨embeddingæœåŠ¡
        # æš‚æ—¶è·³è¿‡é‡å»ºï¼Œç­‰å¾…æ–°æ–‡æ¡£æ·»åŠ æ—¶å†æ„å»º
        print("âš ï¸ éœ€è¦é‡æ–°ç”Ÿæˆembeddingsæ‰èƒ½é‡å»ºç´¢å¼•")
    
    def add_document(self, title: str, content: str, embedding: np.ndarray = None,
                    file_path: str = None, **kwargs) -> str:
        """æ·»åŠ æ–‡æ¡£"""
        doc_id = str(uuid.uuid4())
        
        # æ·»åŠ åˆ°SQLiteæ•°æ®åº“
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        faiss_index = None
        if embedding is not None and faiss is not None and self.index is not None:
            # æ·»åŠ åˆ°FAISSç´¢å¼•
            if len(embedding.shape) == 1:
                embedding = embedding.reshape(1, -1)
            
            # ç¡®ä¿embeddingç»´åº¦æ­£ç¡®
            if embedding.shape[1] != self.dimension:
                print(f"âš ï¸ Embeddingç»´åº¦ä¸åŒ¹é…: {embedding.shape[1]} != {self.dimension}")
                return doc_id
            
            # æ·»åŠ åˆ°FAISSç´¢å¼•
            faiss_index = self.index.ntotal
            self.index.add(embedding.astype('float32'))
            self.document_ids.append(doc_id)
            
            # ä¿å­˜ç´¢å¼•
            self._save_index()
        
        # åŸºç¡€å­—æ®µ
        params = {
            'id': doc_id,
            'title': title,
            'content': content,
            'file_path': file_path,
            'word_count': len(content.split()),
            'char_count': len(content),
            'updated_at': datetime.now().isoformat(),
            'faiss_index': faiss_index
        }
        
        # æ·»åŠ æ‰©å±•å­—æ®µ
        params.update(kwargs)
        
        # æ„å»ºSQL
        columns = ', '.join(params.keys())
        placeholders = ', '.join(['?' for _ in params])
        
        cursor.execute(f'''
            INSERT INTO documents ({columns})
            VALUES ({placeholders})
        ''', list(params.values()))
        
        # æ·»åŠ åˆ°å…¨æ–‡æœç´¢ç´¢å¼•
        cursor.execute('''
            INSERT INTO documents_fts (id, title, content, summary, search_keywords)
            VALUES (?, ?, ?, ?, ?)
        ''', (doc_id, title, content, kwargs.get('summary', ''), kwargs.get('search_keywords', '')))
        
        conn.commit()
        conn.close()
        
        return doc_id
    
    def _save_index(self):
        """ä¿å­˜FAISSç´¢å¼•å’Œå…ƒæ•°æ®"""
        if faiss is None or self.index is None:
            return
        
        try:
            # ä¿å­˜FAISSç´¢å¼•
            faiss.write_index(self.index, self.vector_path)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'document_ids': self.document_ids,
                'dimension': self.dimension,
                'created_at': datetime.now().isoformat()
            }
            
            with open(self.metadata_path, 'wb') as f:
                pickle.dump(metadata, f)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜FAISSç´¢å¼•å¤±è´¥: {e}")
    
    def semantic_search(self, query_embedding: np.ndarray, top_k: int = 10) -> List[Tuple[str, float]]:
        """è¯­ä¹‰æœç´¢"""
        if faiss is None or self.index is None or self.index.ntotal == 0:
            return []
        
        try:
            # ç¡®ä¿æŸ¥è¯¢å‘é‡ç»´åº¦æ­£ç¡®
            if len(query_embedding.shape) == 1:
                query_embedding = query_embedding.reshape(1, -1)
            
            if query_embedding.shape[1] != self.dimension:
                print(f"âš ï¸ æŸ¥è¯¢å‘é‡ç»´åº¦ä¸åŒ¹é…: {query_embedding.shape[1]} != {self.dimension}")
                return []
            
            # æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡
            distances, indices = self.index.search(query_embedding.astype('float32'), min(top_k, self.index.ntotal))
            
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (è·ç¦»è¶Šå°ç›¸ä¼¼åº¦è¶Šé«˜)
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx < len(self.document_ids):
                    # å°†L2è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (0-1)
                    similarity = 1.0 / (1.0 + distance)
                    results.append((self.document_ids[idx], similarity))
            
            return results
            
        except Exception as e:
            print(f"âš ï¸ è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []
    
    def search_documents(self, query: str, query_embedding: np.ndarray = None, 
                        limit: int = 10, search_type: str = 'hybrid') -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£"""
        results = []
        
        # 1. è¯­ä¹‰æœç´¢
        if query_embedding is not None and search_type in ['semantic', 'hybrid']:
            semantic_results = self.semantic_search(query_embedding, limit)
            
            if semantic_results:
                # è·å–æ–‡æ¡£è¯¦æƒ…
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                
                doc_ids = [doc_id for doc_id, _ in semantic_results]
                placeholders = ','.join(['?' for _ in doc_ids])
                
                cursor.execute(f'''
                    SELECT * FROM documents 
                    WHERE id IN ({placeholders}) AND status = 'active'
                ''', doc_ids)
                
                doc_rows = cursor.fetchall()
                columns = [desc[0] for desc in cursor.description]
                docs_dict = {row[0]: dict(zip(columns, row)) for row in doc_rows}
                
                conn.close()
                
                # æŒ‰ç›¸ä¼¼åº¦æ’åº
                for doc_id, similarity in semantic_results:
                    if doc_id in docs_dict:
                        doc = docs_dict[doc_id]
                        doc['similarity_score'] = similarity
                        doc['search_type'] = 'semantic'
                        results.append(doc)
        
        # 2. å…¨æ–‡æœç´¢å’Œå…³é”®è¯æœç´¢
        if search_type in ['fts', 'keyword', 'hybrid']:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æ’é™¤å·²æœ‰ç»“æœ
            existing_ids = [r['id'] for r in results]
            remaining_limit = limit - len(results)
            
            if remaining_limit > 0:
                if search_type in ['fts', 'hybrid']:
                    # å…¨æ–‡æœç´¢
                    exclude_clause = ''
                    params = [query, remaining_limit]
                    
                    if existing_ids:
                        exclude_clause = f"AND d.id NOT IN ({','.join(['?' for _ in existing_ids])})"
                        params = [query] + existing_ids + [remaining_limit]
                    
                    cursor.execute(f'''
                        SELECT d.*, 
                               snippet(documents_fts, 2, '<mark>', '</mark>', '...', 32) as snippet
                        FROM documents_fts fts
                        JOIN documents d ON fts.id = d.id
                        WHERE documents_fts MATCH ?
                          AND d.status = 'active'
                          {exclude_clause}
                        ORDER BY bm25(documents_fts)
                        LIMIT ?
                    ''', params)
                    
                    fts_results = cursor.fetchall()
                    columns = [desc[0] for desc in cursor.description]
                    
                    for row in fts_results:
                        result = dict(zip(columns, row))
                        result['search_type'] = 'fts'
                        result['similarity_score'] = 0.8  # é»˜è®¤FTSåˆ†æ•°
                        results.append(result)
                
                # æ›´æ–°å‰©ä½™é™åˆ¶
                remaining_limit = limit - len(results)
                
                if remaining_limit > 0 and search_type in ['keyword', 'hybrid']:
                    # å…³é”®è¯æœç´¢
                    existing_ids = [r['id'] for r in results]
                    exclude_clause = ''
                    params = [f'%{query}%', f'%{query}%', f'%{query}%', remaining_limit]
                    
                    if existing_ids:
                        exclude_clause = f"AND id NOT IN ({','.join(['?' for _ in existing_ids])})"
                        params = [f'%{query}%', f'%{query}%', f'%{query}%'] + existing_ids + [remaining_limit]
                    
                    cursor.execute(f'''
                        SELECT * FROM documents
                        WHERE (title LIKE ? OR content LIKE ? OR search_keywords LIKE ?)
                          AND status = 'active'
                          {exclude_clause}
                        ORDER BY created_at DESC
                        LIMIT ?
                    ''', params)
                    
                    keyword_results = cursor.fetchall()
                    columns = [desc[0] for desc in cursor.description]
                    
                    for row in keyword_results:
                        result = dict(zip(columns, row))
                        result['search_type'] = 'keyword'
                        result['similarity_score'] = 0.6  # é»˜è®¤å…³é”®è¯åˆ†æ•°
                        results.append(result)
            
            conn.close()
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        
        return results[:limit]
    
    def get_document_by_id(self, doc_id: str) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–æ–‡æ¡£"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM documents WHERE id = ?', (doc_id,))
        row = cursor.fetchone()
        
        if row:
            columns = [desc[0] for desc in cursor.description]
            result = dict(zip(columns, row))
            conn.close()
            return result
        
        conn.close()
        return None
    
    def update_document_embedding(self, doc_id: str, embedding: np.ndarray):
        """æ›´æ–°æ–‡æ¡£çš„embedding"""
        if faiss is None or self.index is None:
            return False
        
        # æŸ¥æ‰¾æ–‡æ¡£åœ¨FAISSç´¢å¼•ä¸­çš„ä½ç½®
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT faiss_index FROM documents WHERE id = ?', (doc_id,))
        row = cursor.fetchone()
        
        if row and row[0] is not None:
            faiss_index = row[0]
            
            # æ›´æ–°FAISSç´¢å¼•ä¸­çš„å‘é‡
            if faiss_index < self.index.ntotal:
                # FAISSä¸æ”¯æŒç›´æ¥æ›´æ–°ï¼Œéœ€è¦é‡å»ºç´¢å¼•
                # è¿™é‡Œæš‚æ—¶è·³è¿‡ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦é‡å»ºæ•´ä¸ªç´¢å¼•
                print("âš ï¸ FAISSä¸æ”¯æŒç›´æ¥æ›´æ–°å‘é‡ï¼Œéœ€è¦é‡å»ºç´¢å¼•")
            
            # æ›´æ–°æ•°æ®åº“æ—¶é—´æˆ³
            cursor.execute('''
                UPDATE documents 
                SET updated_at = ?
                WHERE id = ?
            ''', (datetime.now().isoformat(), doc_id))
            
            conn.commit()
        
        conn.close()
        return True
    
    def delete_document(self, doc_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£ï¼ˆè½¯åˆ é™¤ï¼‰"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            UPDATE documents 
            SET status = 'deleted', updated_at = ?
            WHERE id = ?
        ''', (datetime.now().isoformat(), doc_id))
        
        conn.commit()
        affected = cursor.rowcount
        conn.close()
        
        # æ³¨æ„ï¼šè¿™é‡Œä¸ä»FAISSç´¢å¼•ä¸­åˆ é™¤ï¼Œå› ä¸ºFAISSä¸æ”¯æŒåˆ é™¤
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯èƒ½éœ€è¦å®šæœŸé‡å»ºç´¢å¼•æ¥æ¸…ç†å·²åˆ é™¤çš„æ–‡æ¡£
        
        return affected > 0
    
    def log_search(self, query: str, results_count: int, search_type: str,
                  execution_time: float, session_id: str = None, metadata: str = None):
        """è®°å½•æœç´¢æ—¥å¿—"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO search_logs (id, query, results_count, search_type, 
                                   execution_time, session_id, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (str(uuid.uuid4()), query, results_count, search_type, 
              execution_time, session_id, metadata))
        
        conn.commit()
        conn.close()
    
    def get_document_stats(self) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        stats = {}
        
        # æ€»æ–‡æ¡£æ•°
        cursor.execute("SELECT COUNT(*) FROM documents WHERE status = 'active'")
        stats['total_documents'] = cursor.fetchone()[0]
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        cursor.execute('''
            SELECT file_type, COUNT(*) as count
            FROM documents 
            WHERE status = 'active'
            GROUP BY file_type
            ORDER BY count DESC
        ''')
        stats['by_type'] = dict(cursor.fetchall())
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        cursor.execute('''
            SELECT category, COUNT(*) as count
            FROM documents 
            WHERE status = 'active' AND category IS NOT NULL
            GROUP BY category
            ORDER BY count DESC
        ''')
        stats['by_category'] = dict(cursor.fetchall())
        
        # æ€»å­—æ•°
        cursor.execute("SELECT SUM(word_count) FROM documents WHERE status = 'active'")
        stats['total_words'] = cursor.fetchone()[0] or 0
        
        # FAISSç´¢å¼•ç»Ÿè®¡
        if self.index is not None:
            stats['faiss_vectors'] = self.index.ntotal
            stats['faiss_dimension'] = self.dimension
        else:
            stats['faiss_vectors'] = 0
            stats['faiss_dimension'] = 0
        
        conn.close()
        return stats
    
    def get_collections(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ–‡æ¡£é›†åˆ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('SELECT * FROM document_collections ORDER BY name')
        results = cursor.fetchall()
        columns = [desc[0] for desc in cursor.description]
        
        conn.close()
        return [dict(zip(columns, row)) for row in results]
    
    def create_collection(self, name: str, description: str = None, metadata: str = None) -> str:
        """åˆ›å»ºæ–‡æ¡£é›†åˆ"""
        collection_id = str(uuid.uuid4())
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO document_collections (id, name, description, metadata)
            VALUES (?, ?, ?, ?)
        ''', (collection_id, name, description, metadata))
        
        conn.commit()
        conn.close()
        
        return collection_id
    
    def add_document_to_collection(self, document_id: str, collection_id: str):
        """å°†æ–‡æ¡£æ·»åŠ åˆ°é›†åˆ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR IGNORE INTO document_collection_mapping (document_id, collection_id)
            VALUES (?, ?)
        ''', (document_id, collection_id))
        
        conn.commit()
        conn.close()
    
    def rebuild_faiss_index(self):
        """é‡å»ºFAISSç´¢å¼•"""
        if faiss is None:
            print("âš ï¸ FAISS not available")
            return False
        
        print("ğŸ”„ å¼€å§‹é‡å»ºFAISSç´¢å¼•...")
        
        # åˆ›å»ºæ–°ç´¢å¼•
        new_index = faiss.IndexFlatL2(self.dimension)
        new_document_ids = []
        
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦é‡æ–°ç”Ÿæˆæ‰€æœ‰embeddings
        # å®é™…ä½¿ç”¨æ—¶éœ€è¦è°ƒç”¨embeddingæœåŠ¡
        print("âš ï¸ éœ€è¦é‡æ–°ç”Ÿæˆæ‰€æœ‰æ–‡æ¡£çš„embeddings")
        
        # æ›´æ–°ç´¢å¼•
        self.index = new_index
        self.document_ids = new_document_ids
        
        # ä¿å­˜ç´¢å¼•
        self._save_index()
        
        print("âœ… FAISSç´¢å¼•é‡å»ºå®Œæˆ")
        return True
