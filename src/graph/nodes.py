from typing import List, Optional
from langchain_core.messages import HumanMessage, AIMessage
from src.database.chat_db import ChatDatabase
from src.database.faiss_document_db import FAISSDocumentDatabase
from src.graph.state import AgentState, ToolExecutionResult, RAGSearchResult
from src.memory.smart_memory_manager import SmartMemoryManager
from src.model.chat.base_model import BaseManagedModel
from src.model.embedding import BaseManagedEmbedding
from src.rag import DocumentSearchResult
from src.tools.tool_manager import ToolConfirmationSystem
from src.rag.rag_system import RAGSystem
from src.config.prompt_manager import get_prompt_manager
from src.api.streaming_handler import get_streaming_handler
from langchain_core.tools import BaseTool

class AgentNodes:
    """Agentå·¥ä½œæµèŠ‚ç‚¹"""
    
    def __init__(self, llm: BaseManagedModel, embeddings: BaseManagedEmbedding, chat_db: ChatDatabase, document_db: FAISSDocumentDatabase, tools: Optional[List[BaseTool]], retrival_document_detection_threshold: float = 0.7):
        self.db = chat_db
        self.memory_manager = SmartMemoryManager(llm, chat_db)
        self.tool_system = ToolConfirmationSystem(llm, tools)
        self.llm = llm
        self.rag_system = RAGSystem(document_db, embeddings)
        self.retrival_document_detection_threshold = retrival_document_detection_threshold
        self.prompt_manager = get_prompt_manager()
        self.streaming_handler = get_streaming_handler()
    
    def initialize_session_node(self, state: AgentState) -> AgentState:
        """åˆå§‹åŒ–ä¼šè¯èŠ‚ç‚¹"""
        session_id = state["session_id"]
        
        # åˆå§‹åŒ–ä¼šè¯
        success = self.memory_manager.initialize_session(session_id)
        
        if not success:
            state["error_message"] = f"ä¼šè¯ {session_id} åˆå§‹åŒ–å¤±è´¥"
            return state
        
        # è·å–ä¼šè¯å†å²
        history = self.memory_manager.get_session_history(session_id)
        state["messages"] = history.messages.copy()
        
        print(f"âœ… ä¼šè¯ {session_id} åˆå§‹åŒ–å®Œæˆï¼ŒåŠ è½½äº† {len(state['messages'])} æ¡å†å²è®°å½•")
        
        return state
    
    def save_user_input_node(self, state: AgentState) -> AgentState:
        """ä¿å­˜ç”¨æˆ·è¾“å…¥èŠ‚ç‚¹"""
        session_id = state["session_id"]
        user_input = state["user_input"]
        
        # ä¿å­˜ç”¨æˆ·è¾“å…¥åˆ°æ•°æ®åº“
        self.memory_manager.add_user_message(session_id, user_input)
        
        # æ›´æ–°çŠ¶æ€ä¸­çš„æ¶ˆæ¯
        state["messages"].append(HumanMessage(content=user_input))
        
        print(f"ğŸ’¾ ç”¨æˆ·è¾“å…¥å·²ä¿å­˜: {user_input[:50]}...")
        
        return state
    
    def tool_detection_node(self, state: AgentState) -> AgentState:
        """å·¥å…·æ£€æµ‹èŠ‚ç‚¹"""
        user_input = state["user_input"]
        
        # æ£€æµ‹æ˜¯å¦éœ€è¦å·¥å…·
        detection_result = self.tool_system.tool_matcher.detect_tool_need(user_input)
        
        state["tool_detection_result"] = detection_result
        state["needs_tool"] = detection_result.get("needs_tool", False)
        
        if state["needs_tool"]:
            # å‘é€å·¥å…·æ£€æµ‹äº‹ä»¶åˆ°æµå¼è¾“å‡º
            self.streaming_handler.tool_detected({
                "tool_name": detection_result.get("tool_name", "unknown"),
                "description": detection_result.get("description", ""),
                "confidence": detection_result.get("confidence", 0.0),
                "parameters": detection_result.get("suggested_args", {})
            })
            
            print(f"ğŸ”§ æ£€æµ‹åˆ°éœ€è¦å·¥å…·: {detection_result.get('tool_name', 'unknown')}")
            print(f"ğŸ¯ ç½®ä¿¡åº¦: {detection_result.get('confidence', 0.0):.2f}")
        else:
            print(f"ğŸ’­ æ— éœ€å·¥å…·ï¼Œç›´æ¥LLMå›ç­”")
        
        return state
    
    def tool_execution_node(self, state: AgentState) -> AgentState:
        """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹"""
        detection_result = state["tool_detection_result"]
        tool_name = detection_result.get("tool_name")
        suggested_args = detection_result.get("suggested_args", {})
        confidence = detection_result.get("confidence", 0.0)
        
        # æ£€æŸ¥ç½®ä¿¡åº¦
        if confidence < 0.7:
            print(f"âš ï¸ å·¥å…·æ£€æµ‹ç½®ä¿¡åº¦è¿‡ä½ ({confidence:.2f})ï¼Œè·³è¿‡å·¥å…·æ‰§è¡Œ")
            state["needs_tool"] = False
            return state
        
        # æ˜¾ç¤ºå·¥å…·ä¿¡æ¯å¹¶ç¡®è®¤
        tool_schema = self.tool_system.tool_matcher.get_tool_schema(tool_name)
        if not tool_schema:
            print(f"âŒ æœªæ‰¾åˆ°å·¥å…·: {tool_name}")
            state["needs_tool"] = False
            return state
        
        # ç¡®è®¤å·¥å…·æ‰§è¡Œ - å‘é€åˆ°å‰ç«¯ç¡®è®¤
        # å‘é€å·¥å…·ç¡®è®¤éœ€æ±‚äº‹ä»¶
        self.streaming_handler.tool_confirmation_needed(
            tool_name=tool_name,
            tool_schema=tool_schema,
            suggested_args=suggested_args,
            confidence=confidence
        )
        
        print(f"ğŸ”” ç­‰å¾…ç”¨æˆ·ç¡®è®¤å·¥å…·æ‰§è¡Œ: {tool_name}")
        
        # ç­‰å¾…å‰ç«¯ç¡®è®¤ï¼ˆæœ€å¤šç­‰å¾…60ç§’ï¼‰
        confirmation_result = self.streaming_handler.wait_for_tool_confirmation(
            state["session_id"], timeout=60
        )
        
        if confirmation_result and confirmation_result.get("confirmed"):
            # ç”¨æˆ·ç¡®è®¤æ‰§è¡Œå·¥å…·
            final_args = confirmation_result.get("tool_args", suggested_args)
            print(f"âœ… ç”¨æˆ·ç¡®è®¤æ‰§è¡Œå·¥å…·: {tool_name}")
            
            # å‘é€å·¥å…·æ‰§è¡Œå¼€å§‹äº‹ä»¶
            self.streaming_handler.tool_execution_start(tool_name)
            
            # æ‰§è¡Œå·¥å…·
            success, result = self.tool_system.execute_tool(tool_name, final_args)
            
            # å‘é€å·¥å…·æ‰§è¡Œå®Œæˆäº‹ä»¶
            self.streaming_handler.tool_execution_complete(tool_name, success, result)
            
            # åˆ›å»ºå·¥å…·æ‰§è¡Œç»“æœ
            tool_result = ToolExecutionResult(
                success=success,
                tool_name=tool_name,
                tool_args=final_args,
                result=result,
                confidence=confidence
            )
            
            state["tool_execution_result"] = tool_result
            
            if success:
                print(f"âœ… å·¥å…·æ‰§è¡ŒæˆåŠŸ: {result}")
                
                # ä¿å­˜å·¥å…·æ‰§è¡Œç»“æœåˆ°æ•°æ®åº“
                self.memory_manager.add_tool_message(
                    state["session_id"],
                    result,
                    tool_name,
                    final_args
                )
                
                # è®¾ç½®æœ€ç»ˆå“åº”
                state["final_response"] = f"å·¥å…·æ‰§è¡Œç»“æœ: {result}"
                
            else:
                print(f"âŒ å·¥å…·æ‰§è¡Œå¤±è´¥: {result}")
                state["error_message"] = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {result}"
                state["needs_tool"] = False
        else:
            # ç”¨æˆ·å–æ¶ˆæˆ–è¶…æ—¶
            if confirmation_result is None:
                print("â° å·¥å…·ç¡®è®¤è¶…æ—¶ï¼Œå–æ¶ˆæ‰§è¡Œ")
                self.streaming_handler.add_event("tool_confirmation_timeout", {
                    "tool_name": tool_name,
                    "message": "å·¥å…·ç¡®è®¤è¶…æ—¶ï¼Œå·²å–æ¶ˆæ‰§è¡Œ"
                })
            else:
                print("âŒ ç”¨æˆ·å–æ¶ˆå·¥å…·æ‰§è¡Œ")
                self.streaming_handler.add_event("tool_confirmation_cancelled", {
                    "tool_name": tool_name,
                    "message": "ç”¨æˆ·å–æ¶ˆå·¥å…·æ‰§è¡Œ"
                })
            state["needs_tool"] = False
        
        return state
    
    def llm_response_node(self, state: AgentState) -> AgentState:
        """LLMå“åº”èŠ‚ç‚¹"""
        session_id = state["session_id"]
        messages = state["messages"].copy()
        
        try:
            # å‘é€LLMå“åº”å¼€å§‹äº‹ä»¶
            print(f"ğŸ”„ å‘é€LLMå“åº”å¼€å§‹äº‹ä»¶åˆ°æµå¼å¤„ç†å™¨...")
            self.streaming_handler.llm_response_start()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰RAGä¸Šä¸‹æ–‡
            rag_result = state.get("rag_search_result")
            if rag_result and rag_result.has_relevant_docs:
                # æ„å»ºç‰¹æ®Šçš„promptæ¥å¼•å¯¼LLMåˆ†ææ–‡æ¡£ç›¸å…³æ€§
                structured_prompt = self._build_structured_rag_prompt(
                    user_question=state["user_input"],
                    documents=rag_result.documents
                )
                
                # æ›¿æ¢æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¸ºç»“æ„åŒ–prompt
                messages[-1] = HumanMessage(content=structured_prompt)
                
                print(f"ğŸ“– ä½¿ç”¨ç»“æ„åŒ–RAG promptç”Ÿæˆå“åº”")
            else:
                print(f"ğŸ¤– ç›´æ¥ä½¿ç”¨LLMç”Ÿæˆå“åº”")
            
            # è·å–LLMå“åº”
            response = self.llm.invoke(messages)
            ai_response = response.content
            
            # å¦‚æœä½¿ç”¨äº†RAGï¼Œå¤„ç†ç»“æ„åŒ–å“åº”
            if rag_result and rag_result.has_relevant_docs:
                final_response = self._process_structured_response(ai_response, rag_result.documents)
            else:
                final_response = ai_response
            
            # å‘é€LLMå“åº”å®Œæˆäº‹ä»¶
            print(f"ğŸ”„ å‘é€LLMå“åº”å®Œæˆäº‹ä»¶åˆ°æµå¼å¤„ç†å™¨...")
            self.streaming_handler.llm_response_complete(final_response)
            
            # ä¿å­˜AIå“åº”
            self.memory_manager.add_ai_message(session_id, final_response)
            
            # æ›´æ–°çŠ¶æ€
            state["messages"].append(AIMessage(content=final_response))
            state["final_response"] = final_response
            
            print(f"ğŸ¤– LLMå“åº”: {final_response[:100]}...")
            
        except Exception as e:
            error_msg = f"LLMå“åº”å¤±è´¥: {str(e)}"
            self.streaming_handler.error_occurred(error_msg)
            state["error_message"] = error_msg
            state["final_response"] = "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
            print(f"âŒ {error_msg}")
        
        return state
    
    def finalize_response_node(self, state: AgentState) -> AgentState:
        """æœ€ç»ˆå“åº”èŠ‚ç‚¹"""
        session_id = state["session_id"]
        
        # å¢åŠ æ­¥éª¤è®¡æ•°
        state["step_count"] = state.get("step_count", 0) + 1
        
        # æ˜¾ç¤ºä¼šè¯ä¿¡æ¯
        session_info = self.memory_manager.get_session_info(session_id)
        if session_info:
            print(f"\nğŸ“Š ä¼šè¯ç»Ÿè®¡:")
            print(f"   â€¢ æ¶ˆæ¯æ•°é‡: {session_info['message_count']}")
            print(f"   â€¢ æ–‡æœ¬é•¿åº¦: {session_info['text_length']}")
            print(f"   â€¢ éœ€è¦æ‘˜è¦: {'æ˜¯' if session_info['needs_summary'] else 'å¦'}")
        
        print(f"\nğŸ¯ æœ€ç»ˆå“åº”: {state['final_response']}")
        
        return state
    
    def rag_search_node(self, state: AgentState) -> AgentState:
        """RAGæœç´¢èŠ‚ç‚¹"""
        user_input = state["user_input"]
        session_id = state["session_id"]
        
        print(f"ğŸ” æ­£åœ¨æœç´¢ç›¸å…³æ–‡æ¡£...")
        
        try:
            # æœç´¢ç›¸å…³æ–‡æ¡£
            documents = self.rag_system.search_relevant_documents(
                query=user_input,
                top_k=5,
                session_id=session_id
            )


            documents = list(filter(lambda doc: doc.similarity_score > self.retrival_document_detection_threshold, documents))
            
            # ç”ŸæˆLLMä¸Šä¸‹æ–‡
            context_for_llm = self.rag_system.format_context_for_llm(documents)
            
            # ç”Ÿæˆå¼•ç”¨ä¿¡æ¯
            source_references = self.rag_system.format_source_references(documents)
            
            # åˆ›å»ºRAGæœç´¢ç»“æœ
            rag_result = RAGSearchResult(
                has_relevant_docs=len(documents) > 0,
                documents=documents,
                context_for_llm=context_for_llm,
                source_references=source_references,
                search_query=user_input
            )
            state["rag_search_result"] = rag_result
            
            if documents:
                print(f"ğŸ“š æ‰¾åˆ° {len(documents)} ä¸ªç›¸å…³æ–‡æ¡£")
                for i, doc in enumerate(documents, 1):
                    print(f"   {i}. {doc.title} (ç›¸ä¼¼åº¦: {doc.similarity_score:.2f})")
                    if doc.file_path:
                        print(f"      ğŸ“ {doc.file_path}")
                    if doc.start_line > 0:
                        print(f"      ğŸ“ ç¬¬ {doc.start_line}-{doc.end_line} è¡Œ")
            else:
                print(f"ğŸ“ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå°†ç›´æ¥ä½¿ç”¨LLMå›ç­”")
            
        except Exception as e:
            error_msg = f"RAGæœç´¢å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            state["error_message"] = error_msg
            
            # è®¾ç½®ç©ºçš„RAGç»“æœ
            state["rag_search_result"] = RAGSearchResult(
                has_relevant_docs=False,
                documents=[],
                context_for_llm="",
                source_references="",
                search_query=user_input
            )
        
        return state
    
    def error_handling_node(self, state: AgentState) -> AgentState:
        """é”™è¯¯å¤„ç†èŠ‚ç‚¹"""
        error_msg = state.get("error_message", "æœªçŸ¥é”™è¯¯")
        
        # è®°å½•é”™è¯¯
        print(f"âŒ å¤„ç†é”™è¯¯: {error_msg}")
        
        # è®¾ç½®é”™è¯¯å“åº”
        state["final_response"] = f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜: {error_msg}"
        
        return state
    
    def _build_structured_rag_prompt(self, user_question: str, documents: list[DocumentSearchResult]) -> str:
        """æ„å»ºç»“æ„åŒ–çš„RAG prompt"""
        return self.prompt_manager.get_structured_rag_prompt(user_question, documents)
    
    def _process_structured_response(self, llm_response: str, documents: list) -> str:
        """å¤„ç†ç»“æ„åŒ–çš„LLMå“åº”ï¼Œè½¬æ¢ä¸ºmarkdownæ ¼å¼"""
        import json
        import re
        
        try:
            # å°è¯•è§£æJSONå“åº”
            # å…ˆæ¸…ç†å¯èƒ½çš„markdownä»£ç å—
            json_content = llm_response.strip()
            if json_content.startswith("```json"):
                json_content = re.sub(r'^```json\s*', '', json_content)
                json_content = re.sub(r'\s*```$', '', json_content)
            elif json_content.startswith("```"):
                json_content = re.sub(r'^```\s*', '', json_content)
                json_content = re.sub(r'\s*```$', '', json_content)
            
            response_data = json.loads(json_content)
            
            # æ„å»ºmarkdownå“åº”
            markdown_response = []
            
            # ä¸»è¦å›ç­”
            main_answer = response_data.get("answer_from_llm", "")
            if main_answer:
                markdown_response.append(main_answer)
            
            # æ·»åŠ åˆ†éš”çº¿
            markdown_response.append("\n---\n")
            
            # æ·»åŠ ç›¸å…³æ–‡æ¡£ä¿¡æ¯
            related_doc_ids = response_data.get("related_doc", [])
            doc_answer = response_data.get("answer_from_provided_doc", "")
            
            if related_doc_ids and doc_answer:
                markdown_response.append("### ğŸ“š åŸºäºæ–‡æ¡£çš„å›ç­”")
                markdown_response.append(doc_answer)
                markdown_response.append("")
            
            if related_doc_ids:
                markdown_response.append("### ğŸ“– ç›¸å…³æ–‡æ¡£")
                
                # åˆ›å»ºæ–‡æ¡£IDåˆ°æ–‡æ¡£å¯¹è±¡çš„æ˜ å°„
                doc_map = {doc.document_id: doc for doc in documents}
                
                for doc_id in related_doc_ids:
                    if doc_id in doc_map:
                        doc = doc_map[doc_id]
                        markdown_response.append(f"- **{doc.title}**")
                        if doc.file_path:
                            markdown_response.append(f"  - ğŸ“ æ–‡ä»¶: `{doc.file_path}`")
                        if hasattr(doc, 'start_line') and doc.start_line > 0:
                            markdown_response.append(f"  - ğŸ“ ä½ç½®: ç¬¬ {doc.start_line}-{doc.end_line} è¡Œ")
                        markdown_response.append(f"  - ğŸ¯ ç›¸ä¼¼åº¦: {doc.similarity_score:.3f}")
                        markdown_response.append("")
            else:
                markdown_response.append("### â„¹ï¸ æ–‡æ¡£ä¿¡æ¯")
                markdown_response.append("æœªæ‰¾åˆ°ä¸é—®é¢˜ç›´æ¥ç›¸å…³çš„æ–‡æ¡£ï¼Œå›ç­”ä¸»è¦åŸºäºAIçš„é€šç”¨çŸ¥è¯†ã€‚")
            
            return "\n".join(markdown_response)
            
        except (json.JSONDecodeError, KeyError) as e:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸå§‹å“åº”
            print(f"âš ï¸ æ— æ³•è§£æç»“æ„åŒ–å“åº”ï¼Œè¿”å›åŸå§‹å†…å®¹: {e}")
            
            # æ„å»ºåŸºæœ¬çš„markdownæ ¼å¼
            markdown_response = [llm_response]
            markdown_response.append("\n---\n")
            markdown_response.append("### ğŸ“– ç›¸å…³æ–‡æ¡£")
            
            for i, doc in enumerate(documents, 1):
                markdown_response.append(f"{i}. **{doc.title}**")
                if doc.file_path:
                    markdown_response.append(f"   - ğŸ“ `{doc.file_path}`")
                markdown_response.append(f"   - ğŸ¯ ç›¸ä¼¼åº¦: {doc.similarity_score:.3f}")
                markdown_response.append("")
            
            return "\n".join(markdown_response)
