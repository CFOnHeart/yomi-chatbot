import time
import json
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from src.database.faiss_document_db import FAISSDocumentDatabase
from src.embeddings.azure_openai_embeddings import get_azure_openai_embeddings
from langchain_openai.embeddings import AzureOpenAIEmbeddings
from dataclasses import dataclass

@dataclass
class DocumentSearchResult:
    """æ–‡æ¡£æœç´¢ç»“æœ"""
    document_id: str
    title: str
    content: str
    file_path: str
    start_line: int
    end_line: int
    similarity_score: float
    search_type: str
    snippet: str = ""
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

class RAGSystem:
    """æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ - åŸºäºFAISSçš„ç‰ˆæœ¬"""
    
    def __init__(self, document_db: FAISSDocumentDatabase = None, 
                 embeddings: AzureOpenAIEmbeddings = None):
        self.document_db = document_db or FAISSDocumentDatabase()
        self.embeddings = embeddings or get_azure_openai_embeddings()
        self.similarity_threshold = 0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
        
    def search_relevant_documents(self, query: str, top_k: int = 5, 
                                session_id: str = None) -> List[DocumentSearchResult]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        start_time = time.time()
        
        # 1. ç”ŸæˆæŸ¥è¯¢çš„embedding
        query_embedding = None
        try:
            if self.embeddings:
                query_embedding = self.embeddings.embed_query(query)
                query_embedding = np.array(query_embedding)
        except Exception as e:
            print(f"âš ï¸ ç”ŸæˆæŸ¥è¯¢embeddingå¤±è´¥: {e}")
        
        # 2. ä½¿ç”¨FAISSè¿›è¡Œè¯­ä¹‰æœç´¢å’Œä¼ ç»Ÿæœç´¢
        search_type = 'semantic' if query_embedding is not None else 'hybrid'
        results = self.document_db.search_documents(
            query=query,
            query_embedding=query_embedding,
            limit=top_k * 2,
            search_type=search_type
        )
        
        # 3. è½¬æ¢ä¸ºæœç´¢ç»“æœå¯¹è±¡
        search_results = []
        for result in results[:top_k]:
            search_result = DocumentSearchResult(
                document_id=result['id'],
                title=result['title'],
                content=result['content'],
                file_path=result.get('file_path', ''),
                start_line=result.get('start_line') or 0,
                end_line=result.get('end_line') or 0,
                similarity_score=result.get('similarity_score', 0.0),
                search_type=result.get('search_type', 'hybrid'),
                snippet=result.get('snippet', ''),
                metadata={
                    'category': result.get('category'),
                    'tags': result.get('tags'),
                    'author': result.get('author'),
                    'created_at': result.get('created_at'),
                    'file_type': result.get('file_type'),
                    'word_count': result.get('word_count'),
                    'char_count': result.get('char_count')
                }
            )
            search_results.append(search_result)
        
        # 4. è®°å½•æœç´¢æ—¥å¿—
        execution_time = time.time() - start_time
        self.document_db.log_search(
            query=query,
            results_count=len(search_results),
            search_type='rag_faiss',
            execution_time=execution_time,
            session_id=session_id
        )
        
        return search_results
    
    def add_document(self, title: str, content: str, file_path: str = None, 
                    **metadata) -> str:
        """æ·»åŠ æ–‡æ¡£åˆ°RAGç³»ç»Ÿ"""
        # ç”Ÿæˆembedding
        embedding = None
        try:
            if self.embeddings:
                embedding_vector = self.embeddings.embed_documents([content])[0]
                embedding = np.array(embedding_vector)
        except Exception as e:
            print(f"âš ï¸ ç”Ÿæˆembeddingå¤±è´¥: {e}")
        
        # æ·»åŠ åˆ°æ•°æ®åº“
        doc_id = self.document_db.add_document(
            title=title,
            content=content,
            embedding=embedding,
            file_path=file_path,
            **metadata
        )
        
        return doc_id
    
    def format_context_for_llm(self, documents: List[DocumentSearchResult]) -> str:
        """ä¸ºLLMæ ¼å¼åŒ–ä¸Šä¸‹æ–‡"""
        if not documents:
            return ""
        
        context_parts = []
        context_parts.append("=== ç›¸å…³æ–‡æ¡£å†…å®¹ ===")
        
        for i, doc in enumerate(documents, 1):
            context_parts.append(f"\nã€æ–‡æ¡£ {i}ã€‘")
            context_parts.append(f"æ ‡é¢˜: {doc.title}")
            if doc.file_path:
                context_parts.append(f"æ–‡ä»¶: {doc.file_path}")
            if doc.start_line is not None and doc.start_line > 0:
                context_parts.append(f"ä½ç½®: ç¬¬ {doc.start_line}-{doc.end_line} è¡Œ")
            context_parts.append(f"å†…å®¹: {doc.content[:500]}...")  # é™åˆ¶é•¿åº¦
            context_parts.append(f"ç›¸ä¼¼åº¦: {doc.similarity_score:.2f}")
            context_parts.append("-" * 50)
        
        context_parts.append("=== è¯·åŸºäºä»¥ä¸Šæ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ ===")
        
        return "\n".join(context_parts)
    
    def format_source_references(self, documents: List[DocumentSearchResult]) -> str:
        """æ ¼å¼åŒ–å¼•ç”¨æ¥æº"""
        if not documents:
            return ""
        
        references = []
        references.append("\n\nğŸ“š **å‚è€ƒæ–‡æ¡£**:")
        
        for i, doc in enumerate(documents, 1):
            ref_parts = [f"{i}. **{doc.title}**"]
            
            if doc.file_path:
                ref_parts.append(f"   ğŸ“ æ–‡ä»¶: `{doc.file_path}`")
            
            if doc.start_line is not None and doc.start_line > 0:
                ref_parts.append(f"   ğŸ“ ä½ç½®: ç¬¬ {doc.start_line}-{doc.end_line} è¡Œ")
            
            if doc.metadata.get('author'):
                ref_parts.append(f"   ğŸ‘¤ ä½œè€…: {doc.metadata['author']}")
            
            if doc.metadata.get('created_at'):
                ref_parts.append(f"   ğŸ“… åˆ›å»ºæ—¶é—´: {doc.metadata['created_at']}")
            
            if doc.metadata.get('category'):
                ref_parts.append(f"   ğŸ·ï¸ åˆ†ç±»: {doc.metadata['category']}")
            
            references.append("\n".join(ref_parts))
        
        return "\n".join(references)
    
    def add_document_from_file(self, file_path: str, **metadata) -> str:
        """ä»æ–‡ä»¶æ·»åŠ æ–‡æ¡£"""
        from pathlib import Path
        
        path = Path(file_path)
        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            with open(path, 'r', encoding='gbk') as f:
                content = f.read()
        
        # æå–æ–‡ä»¶ä¿¡æ¯
        title = metadata.get('title', path.stem)
        file_type = path.suffix.lower()
        
        # å¦‚æœæ˜¯ä»£ç æ–‡ä»¶ï¼ŒæŒ‰è¡Œåˆ†å‰²å¹¶æ·»åŠ è¡Œå·ä¿¡æ¯
        if file_type in ['.py', '.js', '.ts', '.java', '.cpp', '.c', '.go']:
            lines = content.split('\n')
            metadata.update({
                'file_type': file_type,
                'start_line': 1,
                'end_line': len(lines),
                'search_keywords': self._extract_keywords_from_code(content)
            })
        else:
            metadata.update({
                'file_type': file_type,
                'search_keywords': self._extract_keywords_from_text(content)
            })
        
        return self.add_document(
            title=title,
            content=content,
            file_path=str(path),
            **metadata
        )
    
    def _extract_keywords_from_code(self, code: str) -> str:
        """ä»ä»£ç ä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼Œå¯ä»¥æ ¹æ®éœ€è¦æ”¹è¿›
        import re
        
        # æå–å‡½æ•°åã€ç±»åã€å˜é‡åç­‰
        patterns = [
            r'def\s+(\w+)',      # Pythonå‡½æ•°
            r'class\s+(\w+)',    # Pythonç±»
            r'function\s+(\w+)', # JavaScriptå‡½æ•°
            r'const\s+(\w+)',    # å¸¸é‡
            r'let\s+(\w+)',      # å˜é‡
            r'var\s+(\w+)',      # å˜é‡
        ]
        
        keywords = []
        for pattern in patterns:
            matches = re.findall(pattern, code)
            keywords.extend(matches)
        
        return ', '.join(set(keywords))
    
    def _extract_keywords_from_text(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–ï¼Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„NLPå·¥å…·
        import re
        
        # æå–ä¸­æ–‡è¯æ±‡ï¼ˆé•¿åº¦å¤§äº1çš„ä¸­æ–‡å­—ç¬¦ä¸²ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,}', text)
        
        # æå–è‹±æ–‡å•è¯ï¼ˆé•¿åº¦å¤§äº3çš„è‹±æ–‡å•è¯ï¼‰
        english_words = re.findall(r'[a-zA-Z]{4,}', text)
        
        keywords = list(set(chinese_words + english_words))
        
        return ', '.join(keywords[:20])  # é™åˆ¶å…³é”®è¯æ•°é‡
    
    def get_document_stats(self) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯"""
        return self.document_db.get_document_stats()
    
    def delete_document(self, doc_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        return self.document_db.delete_document(doc_id)
    
    def update_document_embedding(self, doc_id: str):
        """æ›´æ–°æ–‡æ¡£çš„embedding"""
        doc = self.document_db.get_document_by_id(doc_id)
        if not doc:
            return False
        
        try:
            embedding_vector = self.embeddings.embed_documents([doc['content']])[0]
            embedding = np.array(embedding_vector)
            self.document_db.update_document_embedding(doc_id, embedding)
            return True
        except Exception as e:
            print(f"âš ï¸ æ›´æ–°embeddingå¤±è´¥: {e}")
            return False
    
    def rebuild_index(self):
        """é‡å»ºFAISSç´¢å¼•"""
        return self.document_db.rebuild_faiss_index()
