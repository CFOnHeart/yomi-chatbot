"""
æµ‹è¯•æ–°çš„æ¨¡å‹ç®¡ç†ç³»ç»Ÿ
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_model_registry():
    """æµ‹è¯•æ¨¡å‹æ³¨å†Œä»“åº“"""
    print("ğŸ§ª Testing Model Registry...")
    
    from src.global_configuration.model_registry import get_model_registry, get_model
    
    registry = get_model_registry()
    
    # åˆ—å‡ºæ‰€æœ‰æ³¨å†Œçš„æ¨¡å‹
    print(f"ğŸ“‹ Available models: {registry.list_models()}")
    print(f"ğŸ¢ Available providers: {registry.list_providers()}")
    
    # æµ‹è¯•è·å–ç‰¹å®šæ¨¡å‹
    azure_model = get_model("azure/gpt-4o")
    openai_model = get_model("openai/gpt-4o")
    
    if azure_model:
        print(f"âœ… Azure model found: {azure_model}")
        print(f"   Model info: {azure_model.get_model_info()}")
    else:
        print("âŒ Azure model not found")
    
    if openai_model:
        print(f"âœ… OpenAI model found: {openai_model}")
        print(f"   Model info: {openai_model.get_model_info()}")
    else:
        print("âŒ OpenAI model not found")


def test_settings_integration():
    """æµ‹è¯•ä¸settingsçš„é›†æˆ"""
    print("\nğŸ§ª Testing Settings Integration...")
    
    from src.config.settings import get_llm_model
    
    try:
        llm = get_llm_model()
        print(f"âœ… LLM from settings: {llm}")
        print(f"   Type: {type(llm)}")
        
        # æµ‹è¯•invokeæ–¹æ³•
        if hasattr(llm, 'invoke'):
            print("âœ… Model has invoke method")
        else:
            print("âŒ Model missing invoke method")
            
    except Exception as e:
        print(f"âŒ Error getting LLM from settings: {e}")


def test_model_invocation():
    """æµ‹è¯•æ¨¡å‹è°ƒç”¨"""
    print("\nğŸ§ª Testing Model Invocation...")
    
    try:
        from src.config.settings import get_llm_model
        
        llm = get_llm_model()
        
        # æµ‹è¯•ç®€å•çš„è°ƒç”¨
        test_prompt = "Say 'Hello, this is a test from the new model system!'"
        
        print(f"ğŸ”„ Testing model invocation with prompt: {test_prompt}")
        
        # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½ä¼šå› ä¸ºAPIå¯†é’¥æˆ–ç½‘ç»œé—®é¢˜å¤±è´¥ï¼Œè¿™æ˜¯æ­£å¸¸çš„
        try:
            response = llm.invoke(test_prompt)
            print(f"âœ… Model response: {response}")
        except Exception as invoke_error:
            print(f"âš ï¸ Model invocation failed (this might be due to API keys/network): {invoke_error}")
            
    except Exception as e:
        print(f"âŒ Error in model invocation test: {e}")


if __name__ == "__main__":
    print("ğŸš€ Starting Model System Tests...")
    
    test_model_registry()
    test_settings_integration()
    test_model_invocation()
    
    print("\nâœ… Model system tests completed!")
